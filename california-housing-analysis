import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from zlib import crc32 
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
import seaborn as sns
df = pd.read_csv(r"c:\Users\mayz2\OneDrive\Desktop\coleb\housing.csv")
df.info()
print (df.ocean_proximity.value_counts())
print(df.describe())
print(type(df))
print (df.hist(bins=40, figsize=(11,7)))
plt.savefig('plot.png')
def shuffle_and_split_df(df,test_ratio):
    np.random.seed(40)
    random_indices = np.random.permutation(len(df))
    test_set_size = int(len(df) * test_ratio)
    test_random_indices = random_indices[:test_set_size]
    train_random_indices = random_indices[test_set_size:]
    return df.iloc[train_random_indices], df.iloc[test_random_indices]
train_set, test_set = shuffle_and_split_df(df, 0.2)
print(test_set)
crc32(np.int64(10))
def is_identifier_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio*2**32
def split_train_test_with_identifier_hash(df, test_ratio, identifier_column):
    identifiers = df[identifier_column]
    in_test_set = identifiers.apply(lambda id_: is_identifier_in_test_set(id_, test_ratio))
    return df.loc[~in_test_set], df.loc[in_test_set]
df.reset_index()["index"]
df_with_identifier = df
df_with_identifier["identifier"] = df["longitude"]*1000 + df["latitude"]
df_with_identifier
train_set , test_set = split_train_test_with_identifier_hash(df_with_identifier, 0.2, "identifier")
print(train_set)
train_set, test_set = train_test_split(df, test_size=0.2, random_state=40)
df["median_income_categories"] = pd.cut(df["median_income"],
        bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],
        labels=[1, 2 ,3 ,4, 5])
print(df)
print(df["median_income_categories"].value_counts().sort_index().plot.bar())
sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=40)
stratified_split = []
for train_i, test_i in sss.split(df, df["median_income_categories"]):
  stratified_train_set_n = df.iloc[train_i]
  stratified_test_set_n = df.iloc[test_i]
  stratified_split.append([stratified_train_set_n, stratified_test_set_n])
stratified_train_set, stratified_test_set = stratified_split[0]
print(stratified_test_set["median_income_categories"].value_counts()/len(stratified_test_set))
str_train_set, str_test_set = train_test_split(df, 
                                               test_size=0.2, 
                                               stratify=df["median_income_categories"],
                                               random_state=40)
str_test_set = str_test_set.drop("median_income_categories", axis=1)
str_train_set = str_train_set.drop("median_income_categories", axis=1)
#visualize
train = str_train_set
ax = sns.scatterplot(data=train, 
                x="longitude", 
                y="latitude",
                size="population",
                alpha=0.2,
                hue="median_house_value")

sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
#correlations
train = pd.get_dummies(train, drop_first=True)
print(train.corr()["median_house_value"])
sns.scatterplot(data=train, x="median_income",
                y="median_house_value",
                alpha=0.2)
#attribute combonatons
train["rooms_per_house"] = train["total_rooms"]/train["households"]
train["bedrooms_ratio"] = train["total_bedrooms"]/train["total_rooms"]
train["people_per_house"] = train["population"]/train["households"]
print(train)
print(train.corr()["median_house_value"])
#prepare Data
# Start by dropping all unwanted columns at once

import pandas as pd
import numpy as np

# 1. ایجاد/اصلاح ستون ocean_proximity
if 'ocean_proximity' not in train.columns:
    # اگر ستون اصلی وجود ندارد، از داممی‌ها استفاده می‌کنیم
    conditions = []
    choices = []
    
    if 'ocean_proximity_INLAND' in train.columns:
        conditions.append(train['ocean_proximity_INLAND'] == 1)
        choices.append('INLAND')
    
    if 'ocean_proximity_NEAR BAY' in train.columns:
        conditions.append(train['ocean_proximity_NEAR BAY'] == 1)
        choices.append('NEAR BAY')
    
    if 'ocean_proximity_NEAR OCEAN' in train.columns:
        conditions.append(train['ocean_proximity_NEAR OCEAN'] == 1)
        choices.append('NEAR OCEAN')
    
    if 'ocean_proximity_ISLAND' in train.columns:
        conditions.append(train['ocean_proximity_ISLAND'] == 1)
        choices.append('ISLAND')
    
    train['ocean_proximity'] = np.select(conditions, choices, default='INLAND')

# 2. یکسان‌سازی و اصلاح مقادیر
train['ocean_proximity'] = (
    train['ocean_proximity']
    .str.upper()  # تبدیل به حروف بزرگ برای یکسان‌سازی
    .replace({
        '<1H OCEAN': 'NEAR OCEAN',
        'NEAROCEAN': 'NEAR OCEAN',
        'NEAR_OCEAN': 'NEAR OCEAN',
        'CLOSE TO OCEAN': 'NEAR OCEAN',
        'UNKNOWN': 'INLAND',  # تبدیل UNKNOWN به INLAND
        'OTHER': 'INLAND'     # تبدیل OTHER به INLAND
    })
)

# 3. فقط نگه داشتن ۴ مقدار مجاز
valid_categories = ['INLAND', 'NEAR BAY', 'NEAR OCEAN', 'ISLAND']
train['ocean_proximity'] = train['ocean_proximity'].where(
    train['ocean_proximity'].isin(valid_categories),
    'INLAND'  # مقدار پیش‌فرض برای مقادیر نامعتبر
)

# 4. حذف سطرهای با مقادیر نامعتبر (اگر نیاز باشد)
# train = train[train['ocean_proximity'].isin(valid_categories)]

# 5. بررسی نتایج نهایی
print("توزیع نهایی مقادیر:")
print(train['ocean_proximity'].value_counts())

# 6. حذف ستون‌های داممی (اگر وجود دارند و نیاز نیستند)
dummy_cols = [col for col in train.columns if 'ocean_proximity_' in col]
train.drop(dummy_cols, axis=1, inplace=True, errors='ignore')
# حذف ستون‌های داممی
# Start by dropping all unwanted columns at once


# حذف ستون‌های داممی

train_features = train.drop("median_house_value", axis=1)
train_target = train["median_house_value"]
print(train_target)
##Data cleaning
print(train_features.info())
print(train)
print(train.columns.tolist())
print (train.ocean_proximity.value_counts())
most_common = train['ocean_proximity'].mode()[0]  # پرتکرارترین مقدار
train['ocean_proximity'] = train['ocean_proximity'].replace('UNKNOWN', most_common)
print (train_features)
print(train_target)
print (train_features.info())
# remove rows with NAvalues
print(train_features.dropna(subset=["total_bedrooms"]))
print(train_features)


total_bedrooms_median = train_features["total_bedrooms"].median()
# train_features_with_imputed_na_values
train_features["total_bedrooms"] = train_features["total_bedrooms"].fillna(total_bedrooms_median)
print(train_features.info())
train_features["bedrooms_ratio"] = train_features["total_bedrooms"]/train_features["total_rooms"]
print(train_features.info())

